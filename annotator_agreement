#code for computing annotator agreement on ground truth dataset

import pandas as pd
import numpy as np
from sklearn.metrics import cohen_kappa_score

def bootstrap_ci(stat_func, labels_a, labels_b, n_boot=1000, ci=95, random_state=None):
    """
    Bootstrap confidence interval for a given statistic function.
    """
    rng = np.random.default_rng(random_state)
    stats = []
    n = len(labels_a)
    
    for _ in range(n_boot):
        sample_idx = rng.integers(0, n, n)
        stat = stat_func(labels_a.iloc[sample_idx], labels_b.iloc[sample_idx])
        stats.append(stat)
    
    lower = np.percentile(stats, (100 - ci) / 2)
    upper = np.percentile(stats, 100 - (100 - ci) / 2)
    return lower, upper


def analyze_binary_annotations(df, col_a='label_a', col_b='label_b', n_boot=1000):
    """
    Compute raw percent agreement, Cohen's kappa, and label distribution bias
    for two annotators on a binary classification task with Y/N input labels.
    """
    # Drop rows with missing annotations
    df = df.dropna(subset=[col_a, col_b])
    
    # Convert Y/N to binary 1/0
    map_labels = {'Y': 1, 'N': 0}
    labels_a = df[col_a].map(map_labels)
    labels_b = df[col_b].map(map_labels)

    if labels_a.isnull().any() or labels_b.isnull().any():
        raise ValueError("Labels must be 'Y' or 'N' only.")

    # Stat functions
    def raw_agree(a, b): return (a == b).mean()
    def kappa_func(a, b): return cohen_kappa_score(a, b)
    def bias_func(a, b): return a.mean() - b.mean()

    # Compute point estimates
    raw_agreement = raw_agree(labels_a, labels_b)
    kappa = kappa_func(labels_a, labels_b)
    bias_index = bias_func(labels_a, labels_b)

    # Compute bootstrap CIs
    raw_ci = bootstrap_ci(raw_agree, labels_a, labels_b, n_boot)
    kappa_ci = bootstrap_ci(kappa_func, labels_a, labels_b, n_boot)
    bias_ci = bootstrap_ci(bias_func, labels_a, labels_b, n_boot)

    results = {
        'raw_percent_agreement': (raw_agreement, raw_ci),
        'cohens_kappa': (kappa, kappa_ci),
        'label_distribution_bias': (bias_index, bias_ci)
    }
    return results


# Example usage
if __name__ == "__main__":
    data = {
        'label_a': ['Y', 'N', 'Y', 'Y', 'N', 'N', 'Y'],
        'label_b': ['Y', 'N', 'N', 'Y', 'N', 'Y', 'Y']
    }
    df = pd.DataFrame(data)
    
    stats = analyze_binary_annotations(df, n_boot=5000)
    
    print("Binary task agreement stats with 95% CI:")
    for metric, (estimate, ci) in stats.items():
        print(f"{metric}: {estimate:.3f} (95% CI: {ci[0]:.3f}, {ci[1]:.3f})")
